# ITS Voice RAG Bot - PersonaPlex Configuration

# HuggingFace token (required)
# Get yours at: https://huggingface.co/settings/tokens
# Must accept license at: https://huggingface.co/nvidia/personaplex-7b-v1
HF_TOKEN=

# Device configuration
# Options: "cuda", "cpu", or specific device like "cuda:0"
PERSONAPLEX_DEVICE=cuda

# Enable CPU offloading for systems with limited GPU memory
# Requires 'accelerate' package
PERSONAPLEX_CPU_OFFLOAD=false

# Default voice prompt (NATF0-3, NATM0-3, VARF0-4, VARM0-4)
DEFAULT_VOICE_PROMPT=NATF2

# Default text prompt / persona
DEFAULT_TEXT_PROMPT=You are a helpful ITS support assistant. You help users with technology questions clearly and friendly.

# Server configuration
HOST=0.0.0.0
PORT=8998

# RAG configuration (optional, for document retrieval)
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=llama3.1:8b

# -------------------------------------------------------------------------
# Instead of running a local Ollama server you can call a public HuggingFace
# inference endpoint.  Set HF_CHAT_MODEL to the model name (e.g. "gpt2" or
# "tiiuae/falcon-7b-instruct").  If you leave it blank the code will default
# back to whatever is defined by OLLAMA_MODEL.
HF_CHAT_MODEL=
HF_API_URL=https://api-inference.huggingface.co/models
